# Llama-like MoE LLM Simulator

ä¸€ä¸ªæ¨¡æ‹Ÿ Llama-like æ¶æ„ï¼ˆåŒ…å« MoEï¼‰çš„å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½æµ‹è¯•æ¡†æ¶ã€‚

## ğŸ“ é¡¹ç›®ç»“æ„

```
src/
â”œâ”€â”€ cache_utils.py       # KV Cache å®ç°
â”œâ”€â”€ attention.py         # Attention æ¨¡å—ï¼ˆæ”¯æŒ RoPEã€GQAã€KV Cacheï¼‰
â”œâ”€â”€ moe.py              # MoE æ¨¡å—ï¼ˆRouterã€Expertsã€æ··åˆ GPU/CPU æ‰§è¡Œï¼‰
â”œâ”€â”€ layer.py            # Decoder Layerï¼ˆæ•´åˆ Attention + MoEï¼‰
â”œâ”€â”€ model.py            # ä¸»æ¨¡å‹å’Œè¯„æµ‹è„šæœ¬
â”œâ”€â”€ easy_simulate.py    # å…¥å£è„šæœ¬ï¼ˆå…¼å®¹æ—§ç‰ˆï¼‰
â””â”€â”€ README.md           # æœ¬æ–‡ä»¶
```

## ğŸ¯ åŠŸèƒ½ç‰¹æ€§

### 1. **KV Cache æ”¯æŒ**
- å®Œæ•´çš„ KV Cache ç®¡ç†
- æ”¯æŒ Prefill å’Œ Decode ä¸¤ç§æ¨¡å¼
- æŒ‰å±‚å­˜å‚¨å’Œæ£€ç´¢å†å² K/V

### 2. **Attention æ¨¡å—**
- RoPEï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼‰
- GQAï¼ˆGrouped Query Attentionï¼‰æ”¯æŒ
- KV Cache é›†æˆ
- Flash Attention ä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰

### 3. **MoE æ¨¡å—**
- Top-K è·¯ç”±å™¨ï¼ˆæ¨¡æ‹Ÿè·¯ç”±è®¡ç®—å¼€é”€ï¼‰
- é«˜æ•ˆçš„ä¸“å®¶æ‰§è¡Œï¼ˆ3D tensor ä¼˜åŒ–ï¼‰
- **GPU/CPU æ··åˆæ‰§è¡Œ**
  - GPU å‘½ä¸­ï¼šç›´æ¥è®¡ç®—
  - CPU æœªå‘½ä¸­ï¼šæ•°æ®æ¬è¿ + CPU è®¡ç®— + ç»“æœæ¬è¿
- éšæœºé¢„å–æœºåˆ¶

### 4. **Decoder Layer**
- æ ‡å‡† PreNorm ç»“æ„
- åŒæ®‹å·®è¿æ¥
- Attention â†’ MoE æ•°æ®æµä¸²è”

## ğŸš€ å¿«é€Ÿå¼€å§‹

### è¿è¡Œå®Œæ•´æµ‹è¯•

```bash
cd /home/shiyaochang/workspace/tasks/Llama-like-LLM-Simulatior/src
python easy_simulate.py
```

æˆ–ç›´æ¥è¿è¡Œä¸»æ¨¡å‹ï¼š

```bash
python model.py
```

### è¾“å‡ºç¤ºä¾‹

```
============================================================
Llama-like MoE LLM Simulator
============================================================
Hidden Size: 4096
Num Layers: 10
Num Experts: 64
Active Experts per Token: 4
GPU Capacity: 8/64
Prefetch Accuracy: 0.8
============================================================

============================================================
Prefill Simulation (Sequence Length: 256)
============================================================
é¢„çƒ­ä¸­...
é¢„çƒ­ç»“æŸï¼Œå¼€å§‹æµ‹è¯•...

Layer 1/10 å®Œæˆ | GPU Cache: 8 experts
Layer 2/10 å®Œæˆ | GPU Cache: 8 experts
...
Layer 10/10 å®Œæˆ | GPU Cache: 8 experts

============================================================
Prefill æ€»è€—æ—¶: 2.3456 s
å¹³å‡æ¯å±‚è€—æ—¶: 0.2346 s
============================================================

============================================================
Decode Simulation (Prefill: 256, Decode Steps: 10)
============================================================
Prefill é˜¶æ®µï¼šå¡«å…… 256 tokens...
Prefill å®Œæˆï¼ŒKV Cache é•¿åº¦: 256

Decode é˜¶æ®µï¼šç”Ÿæˆ 10 tokens...
Step 1/10 | è€—æ—¶: 0.0123 s | KV Cache: 257 tokens
Step 2/10 | è€—æ—¶: 0.0115 s | KV Cache: 258 tokens
...
Step 10/10 | è€—æ—¶: 0.0118 s | KV Cache: 266 tokens

============================================================
Decode æ€»è€—æ—¶: 0.1200 s
å¹³å‡æ¯ token è€—æ—¶: 0.0120 s
Tokens/s: 83.33
============================================================
```

## âš™ï¸ é…ç½®å‚æ•°

### æ¨¡å‹æ¶æ„

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `hidden_size` | 4096 | éšè—å±‚ç»´åº¦ |
| `num_hidden_layers` | 10 | æ¨¡å‹å±‚æ•° |
| `num_attention_heads` | 32 | æ³¨æ„åŠ›å¤´æ•° |
| `num_key_value_heads` | 32 | KV å¤´æ•°ï¼ˆç”¨äº GQAï¼‰ |
| `max_position_embeddings` | 4096 | æœ€å¤§åºåˆ—é•¿åº¦ |
| `head_dim` | 128 | æ¯ä¸ª head çš„ç»´åº¦ |
| `rope_theta` | 10000.0 | RoPE åŸºæ•° |

### MoE é…ç½®

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `num_experts` | 64 | æ€»ä¸“å®¶æ•°é‡ |
| `num_experts_per_tok` | 4 | æ¯ä¸ª token æ¿€æ´»çš„ä¸“å®¶æ•° |
| `intermediate_size` | 16384 | MLP ä¸­é—´ç»´åº¦ |
| `use_router_logits` | False | æ˜¯å¦ä½¿ç”¨è·¯ç”±å™¨ logitsï¼ˆæ¨¡æ‹Ÿå¼€é”€ï¼‰ |

### æ¨¡æ‹Ÿé…ç½®

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `gpu_capacity` | 8 | GPU èƒ½ç¼“å­˜çš„ä¸“å®¶æ•°é‡ |
| `prefetch_num` | 4 | æ¯æ¬¡é¢„å–çš„ä¸“å®¶æ•°é‡ |
| `prefetch_acc` | 0.8 | é¢„å–å‘½ä¸­ç‡ï¼ˆå†³å®š GPU/CPU æ‰§è¡Œæ¯”ä¾‹ï¼‰ |

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### 1. åˆ›å»ºæ¨¡å‹

```python
from model import LlamaLikeMoEModel

config = {
    "hidden_size": 4096,
    "num_hidden_layers": 10,
    "num_experts": 64,
    "num_experts_per_tok": 4,
    "gpu_capacity": 8,
    "prefetch_acc": 0.8,
    "use_moe": True,
}

model = LlamaLikeMoEModel(config)
model = model.cuda().half()
model.eval()
```

### 2. Prefill æ¨¡å¼

```python
from model import run_prefill_simulation

prefill_time = run_prefill_simulation(
    model=model,
    config=config,
    num_layers=10,
    seq_len=256,
    use_profiler=True,
)
```

### 3. Decode æ¨¡å¼

```python
from model import run_decode_simulation

decode_time = run_decode_simulation(
    model=model,
    config=config,
    num_layers=10,
    prefill_seq_len=256,
    decode_steps=10,
    use_profiler=True,
)
```

## ğŸ”§ æ¨¡å—è¯´æ˜

### `cache_utils.py`

- `SimpleKVCache`: ç®€åŒ–çš„ KV Cache å®ç°
  - `update()`: æ›´æ–°ç¼“å­˜
  - `get_seq_length()`: è·å–åºåˆ—é•¿åº¦
  - `get()`: è·å–ç¼“å­˜çš„ K/V
  - `reset()`: æ¸…ç©ºç¼“å­˜

### `attention.py`

- `RotaryEmbedding`: RoPE ä½ç½®ç¼–ç 
- `LlamaLikeAttention`: Llama-like Attention æ¨¡å—
  - æ”¯æŒ KV Cache
  - æ”¯æŒ GQA
  - Flash Attention ä¼˜åŒ–

### `moe.py`

- `LlamaLikeExpert`: æ ‡å‡† SwiGLU ä¸“å®¶
- `TopKRouter`: Top-K è·¯ç”±å™¨
- `MoEExperts`: ä¸“å®¶é›†åˆï¼ˆ3D tensor ä¼˜åŒ–ï¼‰
- `StochasticMoELayer`: éšæœº MoE å±‚ï¼ˆæ”¯æŒ GPU/CPU æ··åˆæ‰§è¡Œï¼‰

### `layer.py`

- `LlamaLikeRMSNorm`: RMS Normalization
- `LlamaLikeDecoderLayer`: å®Œæ•´çš„ Decoder Layer
  - Attention + MoE
  - åŒæ®‹å·®è¿æ¥

### `model.py`

- `LlamaLikeMoEModel`: ä¸»æ¨¡å‹
- `run_prefill_simulation()`: Prefill è¯„æµ‹
- `run_decode_simulation()`: Decode è¯„æµ‹
- `main()`: å®Œæ•´çš„æµ‹è¯•æµç¨‹

## ğŸ“Š æ€§èƒ½åˆ†æ

### ä½¿ç”¨ Nsight Profiler

ä»£ç å·²é›†æˆ `NsightProfiler` æ”¯æŒï¼š

```python
# model.py æˆ– easy_simulate.py ä¸­
use_profiler=True
```

### ç›‘æ§æŒ‡æ ‡

- Prefill è€—æ—¶
- Decode è€—æ—¶
- Tokens/s
- GPU Cache å‘½ä¸­æƒ…å†µ
- KV Cache å¤§å°

## ğŸ¨ è®¾è®¡ç†å¿µ

### å‚è€ƒæ¨¡å‹

- **Qwen3 MoE**: è·¯ç”±å™¨è®¾è®¡ã€ä¸“å®¶æ‰§è¡Œã€KV Cache
- **DeepSeek V3**: MoE æ¶æ„ã€GQAã€RoPE

### æ¨¡æ‹Ÿç­–ç•¥

- **è·¯ç”±è®¡ç®—**: ä½¿ç”¨çº¿æ€§å±‚æ¨¡æ‹Ÿè·¯ç”±å¼€é”€
- **ä¸“å®¶é€‰æ‹©**: éšæœºé€‰æ‹©ï¼ˆä¸å—è·¯ç”±å™¨ç»“æœå½±å“ï¼‰
- **GPU/CPU æ‰§è¡Œ**: æ ¹æ®å‘½ä¸­ç‡å†³å®šæ‰§è¡Œä½ç½®
- **æ•°æ®æ¬è¿**: ä½¿ç”¨ dummy buffer æ¨¡æ‹Ÿ PCIe å¸¦å®½æ¶ˆè€—

## ğŸ“š TODO

- [ ] æ·»åŠ æ›´å¤šè¯„æµ‹åœºæ™¯ï¼ˆä¸åŒåºåˆ—é•¿åº¦ã€ä¸åŒå‘½ä¸­ç‡ï¼‰
- [ ] æ”¯æŒå¤š batch æ¨¡æ‹Ÿ
- [ ] æ·»åŠ å¯è§†åŒ–å·¥å…·
- [ ] ä¼˜åŒ– CPU/GPU æ··åˆæ‰§è¡Œç­–ç•¥
- [ ] æ”¯æŒæ›´å¤æ‚çš„ MoE è·¯ç”±ç­–ç•¥

## ğŸ“„ License

MIT License
