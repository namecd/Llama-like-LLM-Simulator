import torch
import torch.nn as nn
import torch.nn.functional as F
import random
import time
from NsightProfiler import NsightProfiler

class LlamaLikeExpert(nn.Module):
    """
    æ ‡å‡†çš„ SwiGLU ä¸“å®¶ç»“æ„ (used in Llama, Mixtral, DeepSeek, etc.)
    åŒ…å« 3 ä¸ªçº¿æ€§å±‚ï¼šGate, Up, Down
    """
    def __init__(self, hidden_size, intermediate_size=None):
        super().__init__()
        # é»˜è®¤ intermediate_size é€šå¸¸æ˜¯ hidden_size çš„ 4 å€æˆ–è€…æ˜¯ 8/3 å€
        if intermediate_size is None:
            intermediate_size = hidden_size * 4
            
        self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False)
        self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=False)
        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)
        self.act_fn = nn.SiLU()

    def forward(self, x):
        # SwiGLU: (SiLU(x @ W_gate) * (x @ W_up)) @ W_down
        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))

class StochasticMoELayer(nn.Module):
    def __init__(self, hidden_size, total_experts, active_experts, gpu_capacity, prefetch_num, prefetch_acc):
        """
        gpu_capacity: æ˜¾å­˜èƒ½å­˜å¤šå°‘ä¸ªä¸“å®¶
        prefetch_num: æ¯æ¬¡éšæœºé¢„å–å¤šå°‘ä¸ªä¸“å®¶
        """
        super().__init__()
        self.hidden_size = hidden_size
        self.total_experts = total_experts
        self.active_experts = active_experts
        self.gpu_capacity = gpu_capacity
        self.prefetch_num = prefetch_num
        self.prefetch_acc = prefetch_acc
        
        # 1. ç‰©ç†èµ„æºæ¨¡æ‹Ÿï¼Œåªæ˜¯åœ¨æ¨¡æ‹Ÿ"ä¼ è¾“"å’Œ"è®¡ç®—"çš„å¼€é”€
        # å®šä¹‰ä¸€ä¸ªæ ‡å‡†çš„ä¸“å®¶å¤§å°ç”¨äºæ¨¡æ‹Ÿå¸¦å®½æ¶ˆè€— (å‡è®¾ FFN æ˜¯ 4å€ hidden)
        self.expert_param_size = hidden_size * (hidden_size * 4) * 2 # ç²—ç•¥ä¼°è®¡å‚æ•°é‡
        # è¿™æ˜¯ä¸€ä¸ªç”¨äºæ¨¡æ‹Ÿæ¬è¿çš„ dummy buffer (CPUç«¯)
        self.cpu_expert_data = torch.randn(self.expert_param_size // 2, dtype=torch.float16, device='cpu')
        
        # æ¨¡æ‹Ÿè®¡ç®—ç”¨çš„æƒé‡ (GPUç«¯ï¼Œå…¬ç”¨ä¸€ä¸ªï¼Œåªä¸ºäº†æµ‹ç®—åŠ›)
        self.common_gpu_expert = LlamaLikeExpert(hidden_size).cuda().half()
        
        # æ¨¡æ‹Ÿcpuè®¡ç®—çš„æƒé‡ (CPUç«¯)
        self.common_cpu_expert = LlamaLikeExpert(hidden_size).half()
        
        self.attn_proj = nn.Linear(hidden_size, 3*hidden_size).cuda().half()    # è½¬ç§»åˆ°GPUä¸Šä¸”ç²¾åº¦è½¬ä¸ºFP16
        self.o_proj = nn.Linear(hidden_size, hidden_size).cuda().half()
        # 2. çŠ¶æ€ç®¡ç† (Set æ¨¡æ‹Ÿç¼“å­˜è¡¨)
        # åˆå§‹éšæœºå¡«å……æ˜¾å­˜
        self.gpu_resident_set = set(random.sample(range(total_experts), min(gpu_capacity, total_experts)))
        self.prefetch_stream = torch.cuda.Stream()

    # --- 1. Attention (å›ºå®šå¼€é”€) ---
    def _compute_attention(self, x):
        # x: [Batch, Seq, Hidden]
        batch, seq, dim = x.shape

        qkv = self.attn_proj(x)
        q,k,v = qkv.chunk(3, dim=-1)
        x_attn = F.scaled_dot_product_attention(
            q.view(batch, seq, 32, -1).transpose(1, 2),
            k.view(batch, seq, 32, -1).transpose(1, 2),
            v.view(batch, seq, 32, -1).transpose(1, 2),
            attn_mask=None,
            dropout_p=0.0,
            is_causal=False
        )

        x = x + self.o_proj(x_attn.transpose(1, 2).reshape(batch, seq, dim))

    def _execute_gpu_hits(self, x, hits):
        for _ in range(hits):
                _ = self.common_gpu_expert(x)
        return x
    
    def _execute_cpu_miss(self, x, misses):
        # æƒ©ç½š 1: æ•°æ®ä» GPU -> CPU
        x_cpu = x.cpu() 
        # æƒ©ç½š 2: CPU æ…¢é€Ÿè®¡ç®—
        for _ in range(misses):
            _ = self.common_cpu_expert(x_cpu)
        # æƒ©ç½š 3: ç»“æœä» CPU -> GPU
        x_cpu = x_cpu.cuda()
        return x + x_cpu
    def _trigger_prefetch(self):
        # æ¨¡æ‹Ÿï¼šéšæœºå†³å®šä¸‹ä¸€æ—¶åˆ»è¦æ¬è°è¿›æ¥
        prefetch_candidates = random.sample(range(self.total_experts), self.prefetch_num)
        
        with torch.cuda.stream(self.prefetch_stream):
            for candidate in prefetch_candidates:
                if candidate not in self.gpu_resident_set:
                    # æ¨¡æ‹Ÿå¸¦å®½æ¶ˆè€—ï¼šå°† expert å¤§å°çš„ dummy æ•°æ®æ¬åˆ° GPU
                    # æ³¨æ„ï¼šè¿™ä¼šå ç”¨ PCIe å¸¦å®½ï¼Œå¯èƒ½å½±å“ä¸Šé¢çš„ misses æ¬è¿é€Ÿåº¦
                    temp = self.cpu_expert_data.to('cuda', non_blocking=True)
                    
                    # æ›´æ–°ç¼“å­˜è¡¨ (ç®€å•çš„éšæœºæ›¿æ¢ç­–ç•¥)
                    if len(self.gpu_resident_set) >= self.gpu_capacity:
                        # éšæœºè¸¢å‡ºä¸€ä¸ª
                        evicted = random.choice(list(self.gpu_resident_set))
                        self.gpu_resident_set.remove(evicted)
                    self.gpu_resident_set.add(candidate)
        
    def forward(self, x):
        self._compute_attention(x)
        
        # --- 3. å‘½ä¸­æ£€æµ‹ä¸æ‰§è¡Œ ---
        hits = int(self.active_experts * self.prefetch_acc)
        misses = self.active_experts - hits
        
        # [ä¼˜åŒ–] tokenè·¯ç”±åˆ°ä¸“å®¶ ---> ä¸“å®¶åŠ è½½è®¡ç®—
        # [è·¯å¾„ A]: GPU å‘½ä¸­ (ç›´æ¥è®¡ç®—)
        if hits > 0:
            # æ¨¡æ‹Ÿè®¡ç®—è€—æ—¶ï¼šè¿è¡Œ len(hits) æ¬¡å…¬ç”¨ä¸“å®¶
            # å®é™…ä¸­æ˜¯å°† token åˆ†ç»„ï¼Œè¿™é‡Œç®€åŒ–ä¸ºæŠŠ Batch æ‰©å¤§æ¨¡æ‹Ÿæ€» FLOPs
            self._execute_gpu_hits(x, hits)
        
        # [è·¯å¾„ B]: GPU æœªå‘½ä¸­ (CPUè®¡ç®—ï¼Œä½†æ˜¯æ•°æ®éœ€è¦ä¸´æ—¶æ¬è¿)
        if misses > 0: 
            self._execute_cpu_miss(x, misses)
            
        # --- 4. éšæœºé¢„å– (Stochastic Prefetch) ---
        if self.prefetch_num > 0:
            self._trigger_prefetch()
        return x

# --- è¯„æµ‹è„šæœ¬ ---
def run_simulation():
    # é…ç½®å‚æ•°
    config = {
        "hidden_size": 4096,
        "total_experts": 64,      # æ€»å…±æœ‰64ä¸ªä¸“å®¶
        "active_experts": 4,      # æ¯ä¸€å±‚ç”¨4ä¸ª
        "gpu_capacity": 8,        # æ˜¾å­˜å¾ˆå°ï¼Œåªèƒ½å­˜8ä¸ª (é«˜ Miss ç‡åœºæ™¯)
        "prefetch_num": 4,         # é¢„å–ä¸‹ä¸€å±‚æ‰€éœ€è¦çš„å…¨éƒ¨ä¸“å®¶
        "prefetch_acc": 0.8
    }
    
    print(f"--- Setting: Capacity {config['gpu_capacity']}/{config['total_experts']} ---")
    model = StochasticMoELayer(**config)
    x = torch.randn(1, 256, 4096).cuda().half() # Batch=1, Seq=4096

    # é¢„çƒ­
    for _ in range(2): model(x)
    torch.cuda.synchronize()

    print("é¢„çƒ­ç»“æŸ")
    # è¿è¡Œ 10 å±‚ (æ¨¡æ‹Ÿä¸€ä¸ª 10 å±‚çš„æ¨¡å‹)
    NsightProfiler.register_layer_hooks(model)
    
    # æ‰‹åŠ¨åŠ«æŒé€»è¾‘æ–¹æ³• (æ˜¾ç¤ºä¸ºè‡ªå®šä¹‰ Tag)
    NsightProfiler.wrap_method(model, '_compute_attention', tag_name='1_Attention_Phase')
    NsightProfiler.wrap_method(model, '_execute_gpu_hits',  tag_name='2_GPU_Expert_Hits')
    NsightProfiler.wrap_method(model, '_execute_cpu_miss',tag_name='âš ï¸_3_CPU_Fallback_Misses')
    NsightProfiler.wrap_method(model, '_trigger_prefetch',  tag_name='ğŸŒŠ_4_Async_Prefetch')
    
    start = time.time()
    for layer_idx in range(10):
        model(x)
        # æ¯ä¸€å±‚ç»“æŸåï¼ŒåŒæ­¥æµï¼Œç¡®ä¿é¢„å–å®Œæˆï¼ˆæˆ–è€…ä¸åŒæ­¥ä»¥æµ‹è¯•æµæ°´çº¿æ•ˆæœï¼‰
        # çœŸå®çš„ MoE Layer ä¹‹é—´æ˜¯ä¸²è¡Œçš„ï¼Œæ‰€ä»¥è¿™é‡Œä¸ç”¨åŒæ­¥ prefetch streamï¼Œ
        # è®©å®ƒå’Œä¸‹ä¸€å±‚çš„ Attention å¹¶è¡Œè·‘
        print(f"Layer {layer_idx + 1} æ¨¡æ‹Ÿç»“æŸ")
    
    torch.cuda.synchronize()
    end = time.time()
    
    print(f"Total Latency (10 layers): {end - start:.4f} s")

if __name__ == "__main__":
    run_simulation()